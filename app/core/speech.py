"""
è¯­éŸ³è¯†åˆ«æœåŠ¡ - æ”¯æŒå®æ—¶è¯­éŸ³è½¬æ–‡å­—å’Œè¯´è¯äººåˆ†ç¦»
"""
import io
import base64
import asyncio
from typing import Optional, Callable, List, Dict, Any
from dataclasses import dataclass, field
from datetime import datetime
import wave
import struct


@dataclass
class TranscriptSegment:
    """è½¬å½•ç‰‡æ®µ"""
    text: str
    speaker: str  # "user" or "other"
    start_time: float
    end_time: float
    confidence: float = 1.0
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())


@dataclass
class ConversationContext:
    """å¯¹è¯ä¸Šä¸‹æ–‡ - ç»´æŠ¤ä¸¤äººå¯¹è¯å†å²"""
    segments: List[TranscriptSegment] = field(default_factory=list)
    max_segments: int = 50  # ä¿ç•™æœ€è¿‘50è½®å¯¹è¯
    
    def add_segment(self, segment: TranscriptSegment):
        self.segments.append(segment)
        # ä¿æŒçª—å£å¤§å°
        if len(self.segments) > self.max_segments:
            self.segments = self.segments[-self.max_segments:]
    
    def get_recent_text(self, n: int = 10) -> str:
        """è·å–æœ€è¿‘nè½®å¯¹è¯çš„æ–‡æœ¬"""
        recent = self.segments[-n:] if len(self.segments) >= n else self.segments
        lines = []
        for seg in recent:
            speaker_label = "ğŸ‘¤ ä½ " if seg.speaker == "user" else "ğŸ§‘ å¯¹æ–¹"
            lines.append(f"{speaker_label}: {seg.text}")
        return "\n".join(lines)
    
    def get_last_other_message(self) -> Optional[str]:
        """è·å–å¯¹æ–¹æœ€åè¯´çš„è¯"""
        for seg in reversed(self.segments):
            if seg.speaker == "other":
                return seg.text
        return None
    
    def get_topics(self) -> List[str]:
        """æå–å¯¹è¯ä¸­çš„å…³é”®è¯é¢˜"""
        # ç®€å•å®ç°ï¼šæå–æœ€è¿‘å¯¹è¯ä¸­çš„å…³é”®è¯
        all_text = " ".join([s.text for s in self.segments[-10:]])
        # è¿™é‡Œå¯ä»¥æ¥å…¥æ›´å¤æ‚çš„NLPæå–
        return [all_text[:50]] if all_text else []
    
    def clear(self):
        self.segments.clear()


class SpeechRecognitionService:
    """
    è¯­éŸ³è¯†åˆ«æœåŠ¡
    
    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. ç¦»çº¿æ¨¡å¼ï¼šä½¿ç”¨ faster-whisper æœ¬åœ°æ¨¡å‹
    2. åœ¨çº¿æ¨¡å¼ï¼šä½¿ç”¨äº‘ç«¯ ASR æœåŠ¡ (å¦‚ Azure, Google)
    """
    
    def __init__(self, mode: str = "offline", model_size: str = "base"):
        """
        åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«æœåŠ¡
        
        Args:
            mode: "offline" ä½¿ç”¨æœ¬åœ° Whisper, "online" ä½¿ç”¨äº‘ç«¯æœåŠ¡
            model_size: Whisper æ¨¡å‹å¤§å° (tiny, base, small, medium, large)
        """
        self.mode = mode
        self.model_size = model_size
        self.model = None
        self.is_initialized = False
        self.context = ConversationContext()
        
        # è¯´è¯äººæ£€æµ‹çŠ¶æ€
        self._current_speaker = "user"
        self._speaker_energy_threshold = 0.02
        
    async def initialize(self):
        """å¼‚æ­¥åˆå§‹åŒ–æ¨¡å‹"""
        if self.is_initialized:
            return
            
        if self.mode == "offline":
            try:
                # å°è¯•åŠ è½½ faster-whisper
                from faster_whisper import WhisperModel
                
                # ä½¿ç”¨ CPU æ¨¡å¼ (ä¹Ÿæ”¯æŒ CUDA)
                self.model = WhisperModel(
                    self.model_size, 
                    device="cpu",
                    compute_type="int8"
                )
                print(f"âœ… Whisper æ¨¡å‹å·²åŠ è½½: {self.model_size}")
                self.is_initialized = True
                
            except ImportError:
                print("âš ï¸ faster-whisper æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                self.mode = "mock"
                self.is_initialized = True
                
            except Exception as e:
                print(f"âš ï¸ Whisper åŠ è½½å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼")
                self.mode = "mock"
                self.is_initialized = True
        else:
            self.is_initialized = True
    
    async def transcribe_audio(
        self, 
        audio_data: bytes, 
        sample_rate: int = 16000,
        detect_speaker: bool = True
    ) -> Optional[TranscriptSegment]:
        """
        è½¬å½•éŸ³é¢‘æ•°æ®
        
        Args:
            audio_data: åŸå§‹éŸ³é¢‘å­—èŠ‚ (PCM 16-bit)
            sample_rate: é‡‡æ ·ç‡
            detect_speaker: æ˜¯å¦æ£€æµ‹è¯´è¯äºº
            
        Returns:
            è½¬å½•ç»“æœç‰‡æ®µ
        """
        if not self.is_initialized:
            await self.initialize()
        
        if not audio_data or len(audio_data) < 1000:
            return None
        
        # æ£€æµ‹è¯´è¯äºº (åŸºäºç®€å•çš„èƒ½é‡æ£€æµ‹)
        speaker = await self._detect_speaker(audio_data) if detect_speaker else "user"
        
        if self.mode == "offline" and self.model:
            return await self._transcribe_whisper(audio_data, sample_rate, speaker)
        else:
            return await self._transcribe_mock(audio_data, speaker)
    
    async def transcribe_base64(
        self, 
        base64_audio: str,
        sample_rate: int = 16000
    ) -> Optional[TranscriptSegment]:
        """ä» Base64 ç¼–ç çš„éŸ³é¢‘è½¬å½•"""
        try:
            audio_bytes = base64.b64decode(base64_audio)
            return await self.transcribe_audio(audio_bytes, sample_rate)
        except Exception as e:
            print(f"Base64 è§£ç å¤±è´¥: {e}")
            return None
    
    async def _transcribe_whisper(
        self, 
        audio_data: bytes, 
        sample_rate: int,
        speaker: str
    ) -> Optional[TranscriptSegment]:
        """ä½¿ç”¨ Whisper æ¨¡å‹è½¬å½•"""
        try:
            # å°† PCM æ•°æ®è½¬æ¢ä¸º WAV æ ¼å¼
            wav_buffer = io.BytesIO()
            with wave.open(wav_buffer, 'wb') as wav_file:
                wav_file.setnchannels(1)
                wav_file.setsampwidth(2)  # 16-bit
                wav_file.setframerate(sample_rate)
                wav_file.writeframes(audio_data)
            
            wav_buffer.seek(0)
            
            # è½¬å½•
            segments, info = self.model.transcribe(
                wav_buffer,
                language="zh",  # ä¸­æ–‡ä¼˜å…ˆ
                vad_filter=True,  # å¯ç”¨ VAD
                vad_parameters=dict(min_silence_duration_ms=500)
            )
            
            # åˆå¹¶æ‰€æœ‰ç‰‡æ®µ
            text_parts = []
            start_time = 0
            end_time = 0
            
            for segment in segments:
                text_parts.append(segment.text.strip())
                if not start_time:
                    start_time = segment.start
                end_time = segment.end
            
            full_text = " ".join(text_parts)
            
            if not full_text.strip():
                return None
            
            result = TranscriptSegment(
                text=full_text,
                speaker=speaker,
                start_time=start_time,
                end_time=end_time,
                confidence=0.9
            )
            
            # æ·»åŠ åˆ°ä¸Šä¸‹æ–‡
            self.context.add_segment(result)
            
            return result
            
        except Exception as e:
            print(f"Whisper è½¬å½•å¤±è´¥: {e}")
            return None
    
    async def _transcribe_mock(
        self, 
        audio_data: bytes,
        speaker: str
    ) -> Optional[TranscriptSegment]:
        """æ¨¡æ‹Ÿè½¬å½• (ç”¨äºæµ‹è¯•)"""
        # æ¨¡æ‹Ÿä¸€äº›å»¶è¿Ÿ
        await asyncio.sleep(0.1)
        
        # æ ¹æ®éŸ³é¢‘é•¿åº¦ç”Ÿæˆæ¨¡æ‹Ÿæ–‡æœ¬
        duration = len(audio_data) / (16000 * 2)  # å‡è®¾ 16kHz 16-bit
        
        mock_phrases = [
            "æˆ‘è§‰å¾—è¿™ä¸ªæƒ³æ³•å¾ˆæœ‰æ„æ€",
            "ä½ è¯´çš„æœ‰é“ç†",
            "è¿™è®©æˆ‘æƒ³èµ·äº†ä¸€å¥è¯",
            "ç¡®å®æ˜¯è¿™æ ·çš„",
            "æˆ‘æœ‰ä¸åŒçš„çœ‹æ³•",
            "è¿™ä¸ªé—®é¢˜å¾ˆå¤æ‚",
            "æˆ‘ä»¬å¯ä»¥æ¢ä¸ªè§’åº¦æ€è€ƒ",
            "è¿™æ­£æ˜¯æˆ‘æƒ³è¯´çš„"
        ]
        
        import random
        text = random.choice(mock_phrases)
        
        result = TranscriptSegment(
            text=text,
            speaker=speaker,
            start_time=0,
            end_time=duration,
            confidence=0.85
        )
        
        self.context.add_segment(result)
        return result
    
    async def _detect_speaker(self, audio_data: bytes) -> str:
        """
        æ£€æµ‹è¯´è¯äºº
        
        ç®€å•å®ç°ï¼šåŸºäºéŸ³é¢‘èƒ½é‡å’Œæ—¶é—´é—´éš”åˆ¤æ–­
        å®é™…ç”Ÿäº§ä¸­åº”ä½¿ç”¨è¯´è¯äººè¯†åˆ«æ¨¡å‹
        """
        # è®¡ç®—éŸ³é¢‘èƒ½é‡
        try:
            samples = struct.unpack(f'{len(audio_data)//2}h', audio_data)
            energy = sum(abs(s) for s in samples) / len(samples) / 32768.0
            
            # ç®€å•çš„äº¤æ›¿æ£€æµ‹é€»è¾‘
            # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„è¯´è¯äººè¯†åˆ«
            if energy > self._speaker_energy_threshold:
                # äº¤æ›¿è¯´è¯äºº
                self._current_speaker = "other" if self._current_speaker == "user" else "user"
            
            return self._current_speaker
            
        except Exception:
            return "user"
    
    def get_context(self) -> ConversationContext:
        """è·å–å½“å‰å¯¹è¯ä¸Šä¸‹æ–‡"""
        return self.context
    
    def reset_context(self):
        """é‡ç½®å¯¹è¯ä¸Šä¸‹æ–‡"""
        self.context.clear()
        self._current_speaker = "user"


# å•ä¾‹
speech_service = SpeechRecognitionService(mode="offline", model_size="base")
